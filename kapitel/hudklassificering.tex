\documentclass[../rapport_MVEX01-11-05]{subfiles}
\begin{document}

\subsection{Klassificering av hudfärg och urskiljning av händer}

För att skilja pixlar med hud från bakgrundspixlar, vilket är ett
centralt problem inom gestigenkänning, används en modell där pixelfärgen antas
bero stokastiskt på om det är en hud- eller
bakgrundspixel. Pixelfärgen antas vara en multivariat normalfördelad
slumpvariabel, där fördelningen beror på om pixeln innehåller hud
eller inte. För att täcka olika hudfärger kan en uppsättning olika
fördelningar för hudfärger användas (\cite{Elmezain08}), där en total
sannolikhetsfördelning sedan är en linjärkombination av dessa. Det
samma kan göras med sannolikhetsfördelningen för pixlar som inte
innehåller hud. I vårt arbete har vi dock avänt en enda fördelning för
hud, och ''ohudsfördelningen'' har antagits vara konstant. Detta då en
uppskattad normalfördelning konsekvent fick alldeles för liten
varians. Detta resultat strämmer överens med Elmezain et al, som även
de hade mindre varians bland pixlar som inte innehöll hud än de som
innehöll hud.

För att bestämma väntevärdesvektorn och kovariansmatrisen i en
uppskattad normalfördelning krävs ett
antal datapunkter, med vilka vi kan uppskatta parametrarna med MLE.

Den mulitvariata fördelningen ges av

\begin{equation*}
  p(x)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}e^{-(x-\mu)^T\Sigma^{-1}(x-\mu)/2}
\end{equation*}

med kovariansmatris $\Sigma$ och väntevärde $\mu$.
Givet $N$ datapunkter $x_i,\:1\leq i\leq N$ ges den logaritmerade
sannolikheten $l(\mu,\Sigma|x_1,{\ldots} ,x_N)$ av

\begin{equation*}
  \begin{aligned}
  l(\mu,\Sigma|x_1,...,x_N) = &-\frac{Nd}{2}\log(2\pi)-\frac{N}{2}\log|\Sigma|\\
                              &-\sum_{i=1}^N\frac{(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)}{2}.
  \end{aligned}
\end{equation*}

Vi kan därefter beräkna derivatan med avseende på $\mu$,

\begin{equation*}
  \frac{\partial l}{\partial \mu}=\sum_{i=1}^N(x_i-\mu)^T\Sigma^{-1},
\end{equation*}

samt derivatan med avseende på $\Sigma^{-1}$,

\begin{equation*}
  \frac{\partial l}{\partial \Sigma^{-1}}=\frac{N}{2}\Sigma
  -\sum_{i=1}^N\frac{(x_i-\mu)(x_i-\mu)^T}{2}.
\end{equation*}

Sätter vi dem i tur och ordning till noll får vi uppskattningarna $\hat\mu$ och $\hat\Sigma$:

\marginpar{Hur då? Det är \emph{inte} uppenbart!}

\begin{gather*}
  \hat\mu    =\frac{1}{N}-\sum_{i=1}^Nx_i\\
  \hat\Sigma =\frac{1}{N-1}\sum_{i=1}^N(x_i-\hat\mu)(x_i-\hat\mu)^T
\end{gather*}

\citeasnoun{Elmezain08} studerade $18\:972$
hud-pixlar och $88\:320$ icke-hud-pixlar, och delade in dessa i fyra
hudfördelningar och en icke-hud-fördelning. De fyra hudfördelningarna
optimerades genom att varje pixel sorterades till en
ursprungsfördelning, varefter parametrarna uppdaterades, och alla
pixlar sorterades om efter de uppkomna fördelningarna --- detta gjordes
iterativt till konvergens.

%Sådana beräkningar gjordes av \citeasnoun{Elmezain08} på 18972
%hud-pixlar och 88320 icke-hud-pixlar. Deras resultat har vi sedan
%använt i vårt arbete.

\subsubsection{SPM --- Skin Probability Map}

För varje pixelfärg kan sannolikheten för att pixeln föreställer hud
respektive ohud bestämmas med fördelningarna ovan, med en korrektion
för att det i allmänhet är troligare att pixlar är ohud än hud. Detta
innebär att om kvoten
\begin{multline*}
\frac{P(\textrm{hud}|\textrm{färg})}{P(\textrm{ickehud}|\textrm{färg})}=\\
=\frac{P(\textrm{färg}|\textrm{hud})P(\textrm{hud})}{P(\textrm{färg})}
 \frac{P(\textrm{färg}}{P(\textrm{ickehud})P(\textrm{ickehud}|\textrm{färg})}=\\
=\frac{P(\textrm{färg}|\textrm{hud})P(\textrm{hud})}{P(\textrm{färg}|\textrm{ickehud})
 P(\textrm{ickehud})}
\end{multline*}
är större än ett, så sägs pixeln föreställa hud. Kvoten
$P(\textrm{hud})/P(\textrm{ickehud})$ sätts för att ange hur mycket vanligare ohud-pixlar
är i bilderna. Då vår sannolikhetsfördelning för ohud är en så grov
uppskattning, har denna kvot fått korrigeras för hand för att optimera
resultatet.

Problemet med denna uträkning är att sannolikheten är tidskrävande att
räkna ut, och därför olämplig att göra för varje pixel i varje bild i
filmen. Detta löses genom att göra beräkningen en gång för alla för
varje tänkbar färg ($256^2$ stycken). Resultatet sparas sedan i en
matris $256\times256$-matris, där färgen blir en koordinat. Denna matris
kallas för en Skin Probability Map, och genom att slå i den kan man
direkt se om en pixel ska klassas som hud eller ej.

\subsubsection{Urskiljning av handen}
När väl hudpixlarna i figuren urskiljts återstår att plocka ut
vad som är handen som ska följas. Det är troligt att man bland
pixlarna fått med ett antal mindre områden som felaktigt bedömts vara
hud. Dessutom är ofta huvudet med i bilden, vilket kan ställa till med
problem. 

Vår lösning på problemet är att först använda den inbyggda
matlabfunktionen \texttt{bwareaopen}, som tar bort sammanhängande 
områden med för få pixlar. Vad som är en lagom gräns kan naturligtvis
variera, men vi använder en gräns på 200 pixlar, vilket visar sig
fungera väldigt bra. Därefter loopar vi igenom bildens kolonner från
höger för att se om kolonnen innehåller någon hudpixel. Den första
pixel som nås på detta sätt väljs till att tillhöra handen, och det
sammanhängande område som pixeln är en del av betraktas som den sökta
handen. Att på detta sätt välja objektet längst till höger av de
intressanta har tidigare gjorts av bl.a. \citeasnoun{Nielsen04}. De använde
sig dock av det högra av de två största sammanhängande objekten,
vilket har fördelen att man inte behöver sätta en manuell gräns för
när ett objekt är ''tillräckligt stort''. En nackdel är att man tvingas
söka igenom hela bilden, samt att den är något svårare att
implementera.

En alternativ metod för att leta från
vänster är lätt att implementera om man vill kunna använda programmet
som vänsterhänt, detta kommer dock att kräva en
separat inlärningsmängd till gestigenkänningen.

Metoden kommer på detta sätt plocka med all hud som hänger samman med
handen, inklusive armen om man använder programmet kortärmad. För att
råda bot på detta kan man identifiera handleden och kapa handen
där. Detta har gjorts av \citeasnoun{Deimel99}, och
görs genom att identifiera den största cirkel
som får plats helt i handen. Denna antas vara centrerad mitt i handen. Då
cirkeln förstoras något är det längsta cirkelsegment som helt får
plats i hudområdet placerat vid handleden, och mittpunktstangenten
till segmentet är då handleden. Handen utgörs då av hudområdet på den
sida om handleden som cirkeln befinner sig på. Denna metod har vi dock
inte implementerat, varför vi förutsätter att användaren använder
långärmad tröja. 
\end{document} 
