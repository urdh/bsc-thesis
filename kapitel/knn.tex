\documentclass[../rapport_MVEX01-11-05]{subfiles}
\begin{document}

\section{$k$-nearest-neighbor}\label{sec:knn}

\knn eller $k$ närmsta grannar är en konceptuellt enkel modellfri
klassificeringsmetod.
Den baseras på prototypobjekt i egenskapsrummet och tilldelar en klass
till en observerad punkt baserat på dess närmsta grannar i egenskapsrummet.

Prototypobjekten är redan klassificerade objekt som väljs ur en inlärningsmängd.
De kan skrivas som
\begin{equation*}
    (\vect{x}_i, l_i)\qquad l_i \in \{ 1,\dotsc,K \}
\end{equation*}
där $l_i$ är en klassetikett och $K$ är antalet klasser.
Prototyperna kan väljas genom mer eller mindre sofistikerade metoder,
%Uppgiften är att bestämma vilken klass ett nytt testobjekt tillhör.

\notes{prototypurval}

Klasserna motsvarar i detta sammanhang olika statiska gester,
eller olika grupperingar i egenskapsrummet. För
varje observerad bildruta undersöker man alltså vilken klass den tillhör.

Avståndet som används är vanligtvis det euklidiska, dvs
\begin{equation*}
    d(i) = \left(\sum_{i=1}^n(x_i-x_0,i)^2\right)^{1/2}
\end{equation*}
där $n$ är dimensionen på egenskapsrummet, $\vect{x}$ är koordinaterna för en
prototyp och $\vect{x_0}$ är observationen. Genom en sökning i
egenskapsrummet letar vi efter de $k$ närmsta grannarna. Om $k$ är större än ett
avgörs klasstillhörigheten genom majoritetsomröstning, dvs. den klass med flest
nära grannar vinner, se figur \ref{fig:knn-overview}.

\begin{figure}[!htb]
    \begin{center}
\includegraphics[width=0.75\textwidth]{bilder/KnnClassification}
    \end{center}
    \caption{kNN-klassificering av den runda observationen i mitten i ett rum
    med två klasser, trianglar och kvadrater. Majoritetsomröstning
    med $k=3$ leder till att den runda klassas som triangel, medan $k=5$ leder
    till klassificering som kvadrat.}
    \label{fig:knn-overview}
\end{figure}

För att metoden ska fungera krävs att alla klasser är representerade med lika
många prototypobjekt, annars får vissa klasser större vikt och kommer med högre
sannolikhet att väljas. Om prototyperna är väl valda avspeglar de den sanna
förderlningen hos klassen.
Den är därför ofta att föredra framför en Gaussian Mixture Model, där
varje klass approximeras av sina anpassade parametrar.
\cite{Hastie09}.

\subsection{Kd-tree och algoritmer för \knn}
Att linjärt söka genom prototypmängden tar lång tid ($\mathcal{O}(d k n)$). Istället kan man,
beroende på mängdens storlek och antalet egenskaper, använda smartare
metoder. Ett tillvägagångssätt är att inte söka de sanna närmsta grannarna,
utan att genom någon slumpmässig metod approximera till önskad noggranhet.
Detta tillåter stora datamängder och hög dimension.

En metod som fungerar bra för dimensioner lägre än ungefär 20 är kd-tree.
Den går ut på att prototypmängden av plan genom egenskapsrummet
delas upp i ett binärt träd. Detta gör sökningen mycket snabbare

%Detta måste givetvis göras en gång per gest, vilket ger oss en kodbok
%per gest; detta eftersom vi har en HMM per gest.
%
%\marginpar{Eller, hur ska vi göra egentligen? En eller flera
% kodböcker? Hur skalar algoritmerna, hur jobbigt blir det? Säger
% någon referens något om detta?}

%FIGUR
%http://en.wikipedia.org/wiki/File:KnnClassification.svg

\end{document}
