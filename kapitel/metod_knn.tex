\documentclass[../rapport_MVEX01-11-05]{subfiles}
\begin{document}

\section{Klassificering av gester} 
\notes{intro?}

\subsection{Konstruktion av prototypmängd för \knn}
%\notes{Tas upp av 3.3.2}
%Genom att plocka ut tre femtedelar av de filmade gesterna fick vi en
%representativ inlärningsmängd. Ur denna tog vi sedan den första bildrutan i
%varje film, då de efterföljande var allt för lika för att tillföra något. Vi
%sparade dessa bilders egenskapsvektorer i en matris --- varje gest 
%fick därmed 24 punkter i
%egenskapsrummet. Till matrisen hade vi även lagt två kolonner som %identifierade
%varje rad, en som specificerade gesten och en som sade vilken filmsekvens den
%hämtats ur. På så vis kunde vi hela tiden spåra felklassificerade gester.

Genom att plocka ut tre femtedelar av de filmade gesterna fick vi en
representativ inlärningsmängd --- de övriga två femtedelarna av filmerna 
användes som testmängd, ur vilken vi endast plockade ut
de fyra första bildrutorna i varje klipp, sammanlagt 96 bilder per gest.
Att ha en oberoende testmängd är viktigt för att kunna säga om klassificeraren är
stark eller inte, vi testade därför även med personer som inte redan 
existerade i vår inlärningsmängd.

%\subsection{Implementering och test av \knn}
% \notes{Förklarar snarare vad KNN är än vad vi gjort?}
%Klassificeringen av testfilmernas gester genomfördes med hjälp av \MATLAB{}s
%inbyggda funktion \texttt{knnsearch}. Med hjälp av den nyligen skapade
%inlärningsmängden som grund kunde vi köra testfilmernas bilder,
%eller närmare bestämt deras egenskapsvektorer genom algoritmen och därmed
%finna de $k$ närmsta prototyperna.
%
%Detta följdes av en majoritetsomröstning där det index som förekom flest gånger
%avgjorde vilken gest som registrerades av programmet. Utöver andelen korrekta
%klassificeringar lagrades även för vilka andra gester den aktuella gesten
%felaktigt identifierades som, detta för att i ett senare skede kunna avgöra hur
%gesternas eventuella snarlikhet påverkade resultatet.

\subsection{Optimering av antal egenskaper och grannar i \knn}
Fram till denna punkt hade vi endast definierat ett antal egenskaper utan att
diskutera huruvida de var användbara som klassificerare eller inte. Att finna
klarhet i denna fråga och därmed ha möjlighet att reducera antalet egenskaper är
fördelaktigt av flera anledningar, dels kan man märkbart förbättra programmets
prestanda då färre egenskaper behöver beräknas och analyseras, och dels kan man
eliminera de egenskaper som kanske rentav försämrar klassificeringen.
Det kan handla om egenskaper som är överdrivet utspridda i
egenskapsrummet eller instabila egenskaper som förorenar datan med slumpmässiga
variationer.

Då vi först tog oss an problemet att finna de bäst klassificerande
egenskaperna identifierade vi tre olika tillvägagångssätt. En variant är att
testa alla möjliga kombinationer av egenskaper,
men detta ger upphov till enorma beräkningskrav. Med 15
egenskaper ger detta sammanlagt $32\,767$ kombinationer, och varje kombination
måste testas mot hela testmängden, i vårt fall 960 bilder, vilket hade tagit
alltför lång tid.
%Man måste alltså bygga ett kd-tree över $30\,000$ gånger och göra totalt $3\,145\,632$ klassificeringar,
%något som skulle ta mycket lång tid.
 
Den andra varianten utnyttjar
möjligheten att vikta de olika egenskaperna för att på så sätt ge de starka en
större inverkan. Man optimerar då
viktfördelningen för att ett nå så bra resultat som möjligt, följt av att man
utesluter de egenskaper som inte bidrar till diskrimineringen. Vi experimenterade
med denna metod lite kort, men konstaterade snart att även den var för 
beräkningsintensiv för våra tillämpningar.

Det tredje tillvägagångssättet, som vi slutligen 
använde, kallas greedy forward selection (se
 figur~\ref{fig:knn-flowchart} för en förklaring av algoritmen).
Resultatet av denna algoritm är en rangordning av egenskaperna.
Vi beräknade denna rangordning för olika värden
av $k$, alltså parametern i \knn-metoden.

Vi lät $k$ variera mellan 1 och 13 och
beräknade andelen korrekta klassificeringar utifrån
först den främsta egenskapen, sedan de två främsta och så vidare upp till
dess att samtliga femton egenskaper var inluderade.
Detta resulterade i att vi hade klassificeringsstyrkan som en funktion
av $k$ och antalet egenskaper (enligt rangordningen),
se figur~\ref{fig:knn-optimering} för en
visualisering av resultaten.

\citeasnoun{Jain97} utförde en undersökning av feature selection-algoritmer,
och fann att forward selection gav bättre resultat än andra mer beräkningsintensiva
metoder så som branch-and-bound.
Här bör nämnas att eftersom algoritmen utför en icke-uttömmande sökning så är
det inte garanterat att den hittar ett globalt optimum \cite{Cover77}.

\begin{figure}[tb]
	\centering 
	\begin{tikzpicture}[node distance = 2cm, auto]
		% nodes
		\node [block] (init) {\textbf{Start}\\Skapa en tom grupp $E$};
		\node [block, below of=init, node distance=3cm] (ident) {Identifiera den egenskap som ger bäst resultat tillsammans med egenskaperna i $E$};
		\node [block, below of=ident, node distance=3cm] (add) {Lägg till denna egenskap i $E$ och ge den en ranking.\\Lägre ranking innebär bättre egenskap};
		\node [decision, right of=ident, node distance=5cm] (finns) {Finns det egenskaper som inte ligger i $E$?};
		\node [block, below of=finns, node distance=5cm] (end) {\textbf{Returnera} egenskaperna, sorterade efter ranking};
		% edges
		\path [line] (init) -- (ident);
		\path [line] (ident) -- (add);
		\path [line] (add) -- (finns);
		\path [line] (finns) -- node [near start] {Ja} (ident);
		\path [line] (finns) -- node [near start] {Nej} (end);
	\end{tikzpicture}
	\caption{Greedy forward selection för att välja de bästa egenskaperna}
	\label{fig:knn-flowchart}
\end{figure}

%Samtidigt lät vi
%i varje uppsättning $k$ variera mellan 1 och 13 för att få
%en uppfattning av dess inverkan (%Trots att algoritmen är krävande att köra bör
%det nämnas att den inte är inte är lika beräkningsintensiv som de två
%förstnämnda, men det ska även nämnas att eftersom algoritmen utför en icke-uttömmande
%sökning så är det inte garanterat att den hittar ett globalt optimum \cite{Cover77}.

\end{document}
