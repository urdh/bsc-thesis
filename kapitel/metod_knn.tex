\documentclass[../rapport_MVEX01-11-05]{subfiles}
\begin{document}

\section{Klassificering av gester} 
%För klassificering av de statiska gesterna (och då även de statiska symboler
%en dynamisk gest är tänkt att bestå av) bestämde vi oss efter konsultation med
%våra handledare för att använda \knn-metoden. Vi hade tidigare sneglat på andra
%metoder som t.ex.~\notes{vilka var det?}, men konstaterade att \knn ofta ger
%tillfredsställande resultat samtidigt som den är intuitiv och relativt enkel
%att implementera.

För klassificering av gesterna användes \knn. För att metoden skulle
ge ett bra resultat var vi tvungna att välja
en bra prototypmängd, och dessutom välja vilka egenskaper vi skulle inkludera
och vilket värde på $k$ som var optimalt.

\subsection{Konstruktion av prototypmängd för \knn}
Inlärningsmängden skapades genom att plocka ut tre femtedelar av filmerna för
respektive gest --- de övriga två femtedelarna av filmerna 
användes som testmängd, ur vilken vi endast plockade ut
de fyra första bildrutorna i varje klipp, sammanlagt 96 bilder per gest.
Att ha en oberoende testmängd är viktigt för att kunna säga om klassificeraren är
effektiv eller inte, vi testade därför även med personer som inte redan 
fanns i vår inlärningsmängd.

Eftersom resultatet vid \knn-klassificering beror starkt på datamängden,
skapade vi en alternativ prototypmängd för realtidstester.
Här lät vi testpersonen filma handen ur flera vinklar,
för att erhålla en mer varierad representation av varje gest.

\subsection{Optimering av antal egenskaper och grannar i \knn}
Fram till denna punkt hade vi endast definierat ett antal egenskaper utan att
diskutera huruvida de var användbara som klassificerare eller inte. Att nå
klarhet i denna fråga, och att därmed ha möjlighet att reducera antalet
egenskaper, är
fördelaktigt av flera anledningar. Dels kan man märkbart förbättra
klassificerarens
tidseffektivitet då färre egenskaper behöver beräknas och analyseras, dels kan man
eliminera de egenskaper som kanske rentav försämrar klassificeringen.
Det kan handla om egenskaper som är överdrivet utspridda i
egenskapsrummet eller instabila egenskaper som förorenar datan med slumpmässiga
variationer.

Då vi först tog oss an problemet att finna de bäst klassificerande
egenskaperna identifierade vi tre olika tillvägagångssätt. En variant är att
testa alla möjliga kombinationer av egenskaper,
men detta ger upphov till enorma beräkningskrav. Med 15
egenskaper ger detta sammanlagt $32\,767$ kombinationer, och varje kombination
måste testas mot hela testmängden, i vårt fall 640 bilder, vilket hade tagit
alltför lång tid.
%Man måste alltså bygga ett kd-tree över $30\,000$ gånger och göra totalt $3\,145\,632$ klassificeringar,
%något som skulle ta mycket lång tid.
 
Den andra varianten utnyttjar
möjligheten att vikta de olika egenskaperna för att på så sätt ge de som bättre
separerar klasserna en
större inverkan. Man optimerar då
viktfördelningen för att nå ett så bra resultat som möjligt. Vi experimenterade
med denna metod lite kort, men konstaterade snart att även den var för 
beräkningsintensiv för våra tillämpningar.

Det tredje tillvägagångssättet, som vi slutligen 
använde, är s.k.~girig optimering. Detta kan delas in i två snarlika 
algoritmer, greedy forward selection respektive backward elimination.
Framåtalgoritmen förklaras 
i figur~\vref{fig:knn-flowchart}; bakåtalgoritmen är snarlik, men börjar 
istället med samtliga egenskaper inkluderade och plockar successivt bort den 
som bidrar minst.
Resultatet av dessa algoritmer är en rangordning av egenskaperna.
Vi beräknade denna rangordning för olika värden
av $k$, alltså parametern i \knn-metoden.

Vi lät $k$ variera mellan 1 och 13 och fick på så sätt ut andelen
korrekta klassificeringar som funktion av $k$ och antal inkluderade egenskaper
för de två algoritmerna. Bilaga~\ref{sec:matlab:knnopt} visar vår
implementering av dessa algoritmer i \MATLAB.

\citeasnoun{Jain97} utförde en undersökning av algoritmer för urval av egenskaper,
och fann att forward selection gav bättre resultat än andra mer beräkningsintensiva
metoder såsom branch-and-bound.
Här bör nämnas att eftersom algoritmen utför en icke-uttömmande sökning så är
det inte garanterat att den hittar ett globalt optimum \cite{Cover77}.

\begin{figure}[tb]
	\centering 
	\begin{tikzpicture}[node distance = 2cm, auto]
		% nodes
		\node [block] (init) {\textbf{Start}\\Skapa en tom grupp $E$};
		\node [block, below of=init, node distance=3cm] (ident) {Identifiera den egenskap som ger bäst resultat tillsammans med egenskaperna i $E$};
		\node [block, below of=ident, node distance=3cm] (add) {Lägg till denna
        egenskap i $E$ och ge den en rankning.\\Lägre rankning innebär bättre egenskap};
		\node [decision, right of=ident, node distance=5cm] (finns) {Finns det egenskaper som inte ligger i $E$?};
		\node [block, below of=finns, node distance=5cm] (end)
        {\textbf{Returnera} egenskaperna, sorterade efter rankning};
		% edges
		\path [line] (init) -- (ident);
		\path [line] (ident) -- (add);
		\path [line] (add) -- (finns);
		\path [line] (finns) -- node [near start] {Ja} (ident);
		\path [line] (finns) -- node [near start] {Nej} (end);
	\end{tikzpicture}
	\caption{Greedy forward selection för att välja de bästa egenskaperna}
	\label{fig:knn-flowchart}
\end{figure}

%Samtidigt lät vi
%i varje uppsättning $k$ variera mellan 1 och 13 för att få
%en uppfattning av dess inverkan (%Trots att algoritmen är krävande att köra bör
%det nämnas att den inte är inte är lika beräkningsintensiv som de två
%förstnämnda, men det ska även nämnas att eftersom algoritmen utför en icke-uttömmande
%sökning så är det inte garanterat att den hittar ett globalt optimum \cite{Cover77}.

\end{document}
