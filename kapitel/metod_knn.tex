\documentclass[../rapport_MVEX01-11-05]{subfiles}
\begin{document}

\subsection{Klassificering av gester} 
\notes{intro?}

\subsubsection{Konstruktion av prototypmängd för \knn}
Genom att plocka ut tre femtedelar av de filmade gesterna fick vi en
representativ inlärningsmängd. Ur denna tog vi sedan den första bildrutan i
varje film, då de efterföljande var allt för lika för att tillföra något. Vi
sparade dessa bilders egenskapsvektorer i en matris --- varje gest 
fick därmed 24 punkter i
egenskapsrummet. Till matrisen hade vi även lagt två kolonner som identifierade
varje rad, en som specificerade gesten och en som sade vilken filmsekvens den
hämtats ur. På så vis kunde vi hela tiden spåra felklassificerade gester.

De övriga två femtedelarna av filmerna användes som testmängd. Här använde vi oss
av de fyra första bildrutorna i varje klipp, sammanlagt 96 bilder per gest.
En oberoende testmängd är viktigt för att kunna säga om klassificeraren är
stark eller inte. Vi testade dessutom med ytterligare personer som
inte redan existerade i vår inlärningsmängd.

%% Förklarar snarare vad KNN är än vad vi gjort?
%\subsubsection{Implementation och test av \knn}
%Klassificeringen av testfilmernas gester genomfördes med hjälp av \MATLAB{}s
%inbyggda funktion \texttt{knnsearch}. Med hjälp av den nyligen skapade
%inlärningsmängden som grund kunde vi köra testfilmernas bilder,
%eller närmare bestämt deras egenskapsvektorer genom algoritmen och därmed
%finna de $k$ närmsta prototyperna.
%
%Detta följdes av en majoritetsomröstning där det index som förekom flest gånger
%avgjorde vilken gest som registrerades av programmet. Utöver andelen korrekta
%klassificeringar lagrades även för vilka andra gester den aktuella gesten
%felaktigt identifierades som, detta för att i ett senare skede kunna avgöra hur
%gesternas eventuella snarlikhet påverkade resultatet.

\subsubsection{Optimering av antal egenskaper och grannar i \knn}
Upp till denna punkt hade vi endast definierat ett antal egenskaper utan att
diskutera huruvida de var användbara som klassificerare eller inte. Att finna
klarhet i denna fråga och därmed ha möjlighet att reducera antalet egenskaper är
fördelaktigt av flera anledningar; dels kan man märkbart förbättra programmets
prestanda då färre egenskaper behöver beräknas och analyseras, och dels kan man
eliminera de egenskaper som kanske rentav försämrar chanserna till en korrekt
klassificering.Det kan handla om egenskaper som är överdrivet utspridda i
vektorrummet eller instabila egenskaper som förorenar datan med slumpmässiga
variationer.

Då vi först tog oss an problemet att finna de bäst klassificerande
egenskaperna identifierade vi tre olika angreppsvinklar. En variant är att helt
enkelt testa olika långa permutationsföljder av egenskaper ---
men att detta ger upphov till enorma beräkningskrav är tydligt. 
Den andra angreppsvinkeln utnyttjar
möjligheten att vikta de olika egenskaperna för att på så sätt ge de starka en
större inverkan. Man optimerar då
viktfördelninger för att ett nå så bra resultat som möjligt, följt av att man
utesluter de egenskaper som inte bidrar till diskrimineringen. Vi experimenterade
med denna metod lite kort, men konstaterade snart att den var allt för 
beräkningsintensiv för våra tillämpningar.

Den tredje metoden --- som vi slutligen kom att
använda --- var utformad på så sätt att vi först identifierade den egenskap som
ensam gav störst antal korrekta klassificeringar. Därefter identifierades
egenskapen som tillsammans med den första gav bäst resultat --- detta
upprepades tills det att samtliga egenskaper var inkluderade. Då detta
inträffade lät vi ändra värdet av $k$, alltså parametern i \knn-metoden, följt av
att vi startade om algoritmen som valde de bästa egenskaperna. Algoritmen
arbetade alltså i linje med flödesschemat i figur~\ref{fig:knn-flowchart}. 
\notes{\cite{Jain97}}
\begin{figure}[tbp]
	\centering 
	%\includegraphics...
	\caption{\notes{FLOWCHART!}}
	\label{fig:knn-flowchart}
\end{figure}

I slutändan hade vi beräknat antalet korrekta klassificeringar utifrån
endast den främsta egenskapen, de två främsta och så vidare upp till
samtliga femton egenskaper. Vi lät även $k$ variera mellan 1 och 13 för att få
en uppfattning av dess inverkan. Trots att algoritmen är krävande att köra bör
det nämnas att den inte är inte är lika beräkningsintensiv som de två
förstnämnda. Det ska även nämnas att eftersom detta är en icke-uttömmande
algoritm så är det inte garanterat att den hittar ett globala optimum \cite{Cover77}.

\end{document}
